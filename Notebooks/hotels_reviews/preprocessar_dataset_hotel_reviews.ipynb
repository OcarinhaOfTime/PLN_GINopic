{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords, pontuação e converte para minusculo"
      ],
      "metadata": {
        "id": "sC_goT1e0SPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import math"
      ],
      "metadata": {
        "id": "3T-nSBbuF6Vi"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    if isinstance(text, str):\n",
        "      text = text.lower()\n",
        "      text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "      text = ' '.join(word for word in text.split()\n",
        "                    if word.isalpha() and word not in stop_words and re.match(r'^[a-z]+$', word))\n",
        "      return text\n",
        "    else:\n",
        "      return ''\n",
        "\n",
        "df = pd.read_csv('files/Hotel_Reviews.csv')\n",
        "\n",
        "df['reviews.text'] = df['reviews.text'].apply(process_text)\n",
        "\n",
        "# if 'rating' in df.columns:\n",
        "#     # df['rating'] = df['rating'].floor()\n",
        "#     df['reviews.ratings'] = df['reviews.ratings'].floor()\n",
        "# else:\n",
        "#     print(\"Column 'reviews.ratings' not found. Check the CSV file or column name.\")\n",
        "\n",
        "df.to_csv('Processed_Hotel_Reviews.csv', index=False)\n",
        "\n",
        "# df[['reviews.text']].to_csv('Processed_Hotel_Reviews.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjQ3qA8h0BmS",
        "outputId": "8a916c96-4658-4078-e488-a28b59f1c772"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o arquivo\n",
        "file_path = 'files/Processed_Hotel_Reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Divide o dataset em treinamento, validação e teste\n",
        "train_df['partition'] = 'train'\n",
        "val_df['partition'] = 'val'\n",
        "test_df['partition'] = 'test'"
      ],
      "metadata": {
        "id": "PXF2zsDWImk6"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combina os dataframes\n",
        "combined_df = pd.concat([train_df, val_df, test_df])\n",
        "\n",
        "# Cria o arquivo .tsv\n",
        "corpus_df = combined_df[['reviews.text', 'partition', 'reviews.rating']]\n",
        "corpus_file_path = 'files/corpus.tsv'\n",
        "corpus_df.to_csv(corpus_file_path, sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "hfYJUtM6JOtb"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai palavras únicas criando o vocabulário\n",
        "word_counter = Counter()\n",
        "\n",
        "for review_text in df['reviews.text'].astype(str):\n",
        "    for word in review_text.split():\n",
        "        if word.isalpha():\n",
        "            word_counter[word] += 1\n",
        "\n",
        "vocabulary = {word for word, count in word_counter.items() if count >= 10}\n",
        "\n",
        "vocabulary_file_path = 'files/vocabulary.txt'\n",
        "with open(vocabulary_file_path, 'w') as vocab_file:\n",
        "    vocab_file.write('\\n'.join(sorted(vocabulary)))\n",
        "\n",
        "# Cria o arquivo metadata.json\n",
        "metadata = {\n",
        "    \"total_documents\": len(df),\n",
        "    \"vocabulary_length\": len(vocabulary),\n",
        "    \"preprocessing-info\": [],\n",
        "    \"labels\": sorted(combined_df['reviews.rating'].unique().tolist()),\n",
        "    \"total_labels\": combined_df['reviews.rating'].nunique(),\n",
        "    \"last-training-doc\": int(train_df.index[-1]) + 1,\n",
        "    \"last-validation-doc\": int(val_df.index[-1]) + 1,\n",
        "}\n",
        "\n",
        "metadata_file_path = 'files/metadata.json'\n",
        "with open(metadata_file_path, 'w') as metadata_file:\n",
        "    json.dump(metadata, metadata_file, indent=4)"
      ],
      "metadata": {
        "id": "jDPlVuhp-ynw"
      },
      "execution_count": 212,
      "outputs": []
    }
  ]
}