{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3T-nSBbuF6Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    if isinstance(text, str):\n",
        "      text = text.lower()\n",
        "      text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "      text = ' '.join(word for word in text.split()\n",
        "                    if word.isalpha() and word not in stop_words and re.match(r'^[a-z]+$', word))\n",
        "      return text\n",
        "    else:\n",
        "      return ''\n",
        "\n",
        "df = pd.read_csv('Hotel_Reviews.csv')\n",
        "\n",
        "# Aplicar o processamento e remover textos vazios\n",
        "df['reviews.text'] = df['reviews.text'].apply(process_text)\n",
        "\n",
        "# Remove linhas onde a coluna 'reviews.text' é vazia\n",
        "df = df[df['reviews.text'].str.strip() != '']\n",
        "\n",
        "# Reinicia os índices após a remoção\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df.to_csv('Processed_Hotel_Reviews.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjQ3qA8h0BmS",
        "outputId": "e40a232e-cd2d-41e6-a7c3-01363bf11d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o arquivo\n",
        "file_path = 'Processed_Hotel_Reviews.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Divide o dataset em treinamento, validação e teste\n",
        "train_df['partition'] = 'train'\n",
        "val_df['partition'] = 'val'\n",
        "test_df['partition'] = 'test'"
      ],
      "metadata": {
        "id": "PXF2zsDWImk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combina os dataframes\n",
        "combined_df = pd.concat([train_df, val_df, test_df])\n",
        "\n",
        "# Cria o arquivo .tsv\n",
        "corpus_df = combined_df[['reviews.text', 'partition', 'reviews.rating']]\n",
        "corpus_file_path = 'corpus.tsv'\n",
        "corpus_df.to_csv(corpus_file_path, sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "hfYJUtM6JOtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai palavras únicas criando o vocabulário\n",
        "word_counter = Counter()\n",
        "\n",
        "for review_text in df['reviews.text'].astype(str):\n",
        "    for word in review_text.split():\n",
        "        if word.isalpha():\n",
        "            word_counter[word] += 1\n",
        "\n",
        "vocabulary = {word for word, count in word_counter.items() if count >= 10}\n",
        "\n",
        "vocabulary_file_path = 'vocabulary.txt'\n",
        "with open(vocabulary_file_path, 'w') as vocab_file:\n",
        "    vocab_file.write('\\n'.join(sorted(vocabulary)))\n",
        "\n",
        "# Remove NaN antes de coletar os labels\n",
        "valid_labels = np.floor(combined_df['reviews.rating'].dropna()).unique().tolist()\n",
        "\n",
        "# Cria o arquivo metadata.json\n",
        "metadata = {\n",
        "    \"total_documents\": len(df),\n",
        "    \"vocabulary_length\": len(vocabulary),\n",
        "    \"preprocessing-info\": [],\n",
        "    \"labels\": sorted(valid_labels),\n",
        "    \"total_labels\": combined_df['reviews.rating'].nunique(),\n",
        "    \"last-training-doc\": int(train_df.index[-1]) + 1,\n",
        "    \"last-validation-doc\": int(val_df.index[-1]) + 1,\n",
        "}\n",
        "\n",
        "metadata_file_path = 'metadata.json'\n",
        "with open(metadata_file_path, 'w') as metadata_file:\n",
        "    json.dump(metadata, metadata_file, indent=4)"
      ],
      "metadata": {
        "id": "jDPlVuhp-ynw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}