{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "824878dd-3950-4a2a-a67d-f9c8d95f9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, sys, json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732134a-7d13-4843-9915-5de8abe5f3aa",
   "metadata": {},
   "source": [
    "# Spellchecker e stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b9c31ad-dc70-4ef8-8ab2-d894a94c413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell_br = SpellChecker(language='pt', distance=1)\n",
    "spell_en = SpellChecker(distance=1)\n",
    "\n",
    "def spell_check(tokens, spell):\n",
    "    return list(filter(None, [spell.correction(t) for t in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ebe06d2d-e147-4ec1-8ea9-c01df8485ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vagner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/vagner/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words_br = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32963678-6441-4a96-a4f3-337f251e220f",
   "metadata": {},
   "source": [
    "# Funcoes de pre-processamento para diferentes idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe08c598-4b1b-42a6-8278-7c6fab6060c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_en(txt):\n",
    "    txt = txt.lower() #All to lower\n",
    "    txt = re.sub(r'\\W+', ' ', txt) #Remove special chars\n",
    "    #tokens = word_tokenize(txt) #Tokenizing text\n",
    "    tokens = txt.split(' ')\n",
    "    tokens = [w for w in tokens if w not in stop_words] # Removing stopwords    \n",
    "    tokens = [w for w in tokens if re.match(r'[a-z]+$', w) != None]\n",
    "    tokens = spell_check(tokens, spell_en)\n",
    "    \n",
    "    txt = ' '.join(tokens)\n",
    "    #txt = txt.replace(\"don't\", \"no\")\n",
    "    #txt = txt.replace(\"'\", ' ')\n",
    "    if len(txt) == 0:\n",
    "        return None\n",
    "\n",
    "    return txt\n",
    "\n",
    "def pre_process_br(txt):\n",
    "    txt = txt.lower() #All to lower\n",
    "    txt = re.sub(r'\\W+', ' ', txt) #Remove special chars\n",
    "    tokens = word_tokenize(txt) #Tokenizing text\n",
    "    tokens = [w for w in tokens if w not in stop_words_br] # Removing stopwords    \n",
    "    tokens = [w for w in tokens if re.match(r'[a-z]+$', w) != None]\n",
    "    tokens = spell_check(tokens, spell_br)\n",
    "    \n",
    "    txt = ' '.join(tokens)\n",
    "    if len(txt) == 0:\n",
    "        return None\n",
    "\n",
    "    return txt\n",
    "\n",
    "def train_test_val_split(df):\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=666)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=666)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecabdac-7f4b-4f7f-8f3b-3b20f7d71f4c",
   "metadata": {},
   "source": [
    "# Pre-Processamento Dataset Reviews de Hoteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "739bbd97-e3c6-4602-aa10-021133ee4326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "combining data\n",
      "saving data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "328"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading\n",
    "print('loading data')\n",
    "\n",
    "data_dir = '../preprocessed_datasets'\n",
    "dataset_dir = f'{data_dir}/hotels_reviews'\n",
    "\n",
    "df = pd.read_csv(f'{dataset_dir}/Hotel_Reviews.csv')\n",
    "df_train, df_val, df_test = train_test_val_split(df)\n",
    "df_train['partition'] = 'train'\n",
    "df_val['partition'] = 'val'\n",
    "df_test['partition'] = 'test'\n",
    "\n",
    "print('combining data')\n",
    "\n",
    "# Combining\n",
    "combined_df = pd.concat([df_train, df_val, df_test])[['reviews.text', 'partition', 'reviews.rating']].dropna()\n",
    "combined_df['txt'] = combined_df['reviews.text'].apply(pre_process_en)\n",
    "combined_df['label'] = combined_df['reviews.rating'].apply(int)\n",
    "corpus_df = combined_df[['txt', 'partition', 'label']].dropna().reset_index(drop=True)\n",
    "labels = sorted(combined_df['label'].unique())\n",
    "\n",
    "print('saving data')\n",
    "\n",
    "#Saving\n",
    "corpus_df.to_csv(f'{dataset_dir}/corpus.tsv', sep='\\t', index=False, header=False)\n",
    "vocab = sorted({w for s in corpus_df.txt for w in s.split(' ')})\n",
    "open(f'{dataset_dir}/vocabulary.txt', 'w+').write('\\n'.join(vocab))\n",
    "\n",
    "metadata = {\n",
    "    \"total_documents\": len(corpus_df),\n",
    "    \"vocabulary_length\": len(vocab),\n",
    "    \"preprocessing-info\": [],\n",
    "    \"labels\": labels,\n",
    "    \"total_labels\": len(labels),\n",
    "    \"last-training-doc\": np.where(corpus_df.partition == 'train')[0][-1],\n",
    "    \"last-validation-doc\": np.where(corpus_df.partition == 'val')[0][-1]\n",
    "}\n",
    "meta_raw = json.dumps(metadata, cls=NpEncoder, indent=4)\n",
    "open(f'{dataset_dir}/metadata.json', 'w+').write(meta_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e9150-e902-4203-8308-578867b566d2",
   "metadata": {},
   "source": [
    "# Pre-Processamento Dataset FakeNewsBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "513a1303-fe87-47cb-aed9-6e6e2d133c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fake</td>\n",
       "      <td>katia abreu diz vai colocar expulsao moldura n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>fake</td>\n",
       "      <td>ray peita bolsonaro conservador fake entrevist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>fake</td>\n",
       "      <td>reinaldo azevedo desmascarado policia federal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fake</td>\n",
       "      <td>relatorio assustador bndes mostra dinheiro pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fake</td>\n",
       "      <td>radialista americano fala sobre pt vendem ilus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>7195</td>\n",
       "      <td>true</td>\n",
       "      <td>jornal britanico acao contra lula lava jato se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>7196</td>\n",
       "      <td>true</td>\n",
       "      <td>temer diz acionou pf cade investigar aumentos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>7197</td>\n",
       "      <td>true</td>\n",
       "      <td>obstaculos politicos temer especialistas ouvid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>7198</td>\n",
       "      <td>true</td>\n",
       "      <td>setembro boa noite aqui estao principais notic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>7199</td>\n",
       "      <td>true</td>\n",
       "      <td>envolvo politica diz brasileiro preso venezuel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index label                                  preprocessed_news\n",
       "0         0  fake  katia abreu diz vai colocar expulsao moldura n...\n",
       "1         1  fake  ray peita bolsonaro conservador fake entrevist...\n",
       "2         2  fake  reinaldo azevedo desmascarado policia federal ...\n",
       "3         3  fake  relatorio assustador bndes mostra dinheiro pub...\n",
       "4         4  fake  radialista americano fala sobre pt vendem ilus...\n",
       "...     ...   ...                                                ...\n",
       "7195   7195  true  jornal britanico acao contra lula lava jato se...\n",
       "7196   7196  true  temer diz acionou pf cade investigar aumentos ...\n",
       "7197   7197  true  obstaculos politicos temer especialistas ouvid...\n",
       "7198   7198  true  setembro boa noite aqui estao principais notic...\n",
       "7199   7199  true  envolvo politica diz brasileiro preso venezuel...\n",
       "\n",
       "[7200 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading\n",
    "data_dir = '../preprocessed_datasets'\n",
    "dataset_dir = f'{data_dir}/BRNews'\n",
    "\n",
    "df = pd.read_csv(f'{dataset_dir}/pre-processed.csv').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75141c0f-1551-4a20-b70f-89fe4e91815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>partition</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>antagonista dilema usa agência estatal sustent...</td>\n",
       "      <td>train</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fausto acedo brando fatio julga governador acr...</td>\n",
       "      <td>train</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dia teatro acessível celebrado vez pais tanta ...</td>\n",
       "      <td>train</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anunciam única categoria posicionou contra lad...</td>\n",
       "      <td>train</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medico afirma lula síndrome amnésia entra pedi...</td>\n",
       "      <td>train</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>frente frente juiz moro intima lula depor próx...</td>\n",
       "      <td>test</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>lava jato completa quatro anos sentenças serio...</td>\n",
       "      <td>test</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7197</th>\n",
       "      <td>faria magistrada rosário anula decisão juiz fe...</td>\n",
       "      <td>test</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>apresentadora na sofre ataque hotel escapa a o...</td>\n",
       "      <td>test</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7199</th>\n",
       "      <td>pode afastar cunha pais decente presidente den...</td>\n",
       "      <td>test</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    txt partition label\n",
       "0     antagonista dilema usa agência estatal sustent...     train  fake\n",
       "1     fausto acedo brando fatio julga governador acr...     train  true\n",
       "2     dia teatro acessível celebrado vez pais tanta ...     train  true\n",
       "3     anunciam única categoria posicionou contra lad...     train  fake\n",
       "4     medico afirma lula síndrome amnésia entra pedi...     train  fake\n",
       "...                                                 ...       ...   ...\n",
       "7195  frente frente juiz moro intima lula depor próx...      test  fake\n",
       "7196  lava jato completa quatro anos sentenças serio...      test  true\n",
       "7197  faria magistrada rosário anula decisão juiz fe...      test  fake\n",
       "7198  apresentadora na sofre ataque hotel escapa a o...      test  fake\n",
       "7199  pode afastar cunha pais decente presidente den...      test  fake\n",
       "\n",
       "[7200 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_val, df_test = train_test_val_split(df)\n",
    "df_train['partition'] = 'train'\n",
    "df_val['partition'] = 'val'\n",
    "df_test['partition'] = 'test'\n",
    "\n",
    "# Combining\n",
    "combined_df = pd.concat([df_train, df_val, df_test])[['preprocessed_news', 'partition', 'label']].dropna()\n",
    "combined_df['txt'] = combined_df['preprocessed_news'].apply(pre_process_br)\n",
    "#combined_df['label'] = combined_df['reviews.rating'].apply(int)\n",
    "corpus_df = combined_df[['txt', 'partition', 'label']].dropna().reset_index(drop=True)\n",
    "labels = sorted(combined_df['label'].unique())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93aa4ca4-ebbd-41f5-8871-e57d5934b974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving\n",
    "corpus_df.to_csv(f'{dataset_dir}/corpus.tsv', sep='\\t', index=False, header=False)\n",
    "vocab = sorted({w for s in corpus_df.txt for w in word_tokenize(s)})\n",
    "open(f'{dataset_dir}/vocabulary.txt', 'w+').write('\\n'.join(vocab))\n",
    "\n",
    "metadata = {\n",
    "    \"total_documents\": len(corpus_df),\n",
    "    \"vocabulary_length\": len(vocab),\n",
    "    \"preprocessing-info\": [],\n",
    "    \"labels\": labels,\n",
    "    \"total_labels\": len(labels),\n",
    "    \"last-training-doc\": np.where(corpus_df.partition == 'train')[0][-1],\n",
    "    \"last-validation-doc\": np.where(corpus_df.partition == 'val')[0][-1]\n",
    "}\n",
    "meta_raw = json.dumps(metadata, cls=NpEncoder, indent=4)\n",
    "open(f'{dataset_dir}/metadata.json', 'w+').write(meta_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ca854-49d4-4dd0-a125-4447be2d23fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
